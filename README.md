# Advanced RAG System

A production-ready Retrieval-Augmented Generation (RAG) system implementing hybrid search, cross-encoder re-ranking, and agentic orchestration via LangGraph.

## Overview

This project implements a state-of-the-art RAG pipeline comparable to systems deployed in production environments at leading AI companies. It demonstrates:

- **Hybrid Retrieval**: Dense (semantic) + Sparse (BM25) search with Reciprocal Rank Fusion
- **Cross-Encoder Re-ranking**: Using BGE-reranker for optimal context selection
- **Agentic Orchestration**: LangGraph-based pipeline for deterministic, reproducible workflows
- **Comprehensive Evaluation**: Ragas metrics + custom metrics (Recall@K, MRR, nDCG)
- **Production Practices**: Professional logging, testing, monitoring, and documentation

## Architecture

```
                 ┌─────────────────────────┐
                 │     User Query          │
                 └─────────────┬───────────┘
                               ↓
                ┌──────────────────────────┐
                │      Preprocessing       │
                └─────────────┬────────────┘
                              ↓
          ┌─────────────────────────────────────────┐
          │         RETRIEVAL PIPELINE              │
          │─────────────────────────────────────────│
          │  1. Dense Embedding Search (BGE-large)  │
          │  2. Sparse Search (BM25)                │
          │  3. Fusion (RRF)                        │
          │  4. Re-ranking (BGE-reranker)           │
          └───────────────────────┬─────────────────┘
                                  ↓
                     ┌──────────────────────┐
                     │   Context Selection  │
                     └─────────┬────────────┘
                               ↓
               ┌────────────────────────────────┐
               │   Generation (Mistral 7B)      │
               └────────────────────────────────┘
                               ↓
              ┌───────────────────────────────────┐
              │   Final Answer + Retrieved Docs   │
              └───────────────────────────────────┘
```

## Features

### Core Components
- **Embeddings**: BGE-large-en-v1.5 for semantic search
- **Vector Store**: FAISS with GPU acceleration
- **Sparse Retrieval**: BM25 via rank-bm25
- **Re-ranker**: BGE-reranker-base cross-encoder
- **LLM**: Mistral-7B-Instruct-v0.2
- **Orchestration**: LangGraph for pipeline management

### Evaluation Metrics
- **Ragas**: Context Precision, Context Recall, Faithfulness, Answer Relevancy
- **Custom**: Recall@K, Mean Reciprocal Rank (MRR), nDCG, Latency

### Production Features
- Professional logging with rotation (loguru)
- GPU memory management
- Comprehensive testing (pytest)
- Type hints throughout
- Docker support
- CI/CD pipeline (GitHub Actions)

## Project Structure

```
advanced-rag/
├── src/
│   ├── __init__.py
│   ├── utils.py              # Logging, GPU management, timing
│   ├── loader.py             # Document loading
│   ├── chunking.py           # Text splitting strategies
│   ├── embeddings.py         # Embedding models & indexing
│   ├── retriever.py          # Dense, sparse, hybrid retrieval
│   ├── reranker.py           # Cross-encoder re-ranking
│   ├── llm.py                # LLM inference
│   ├── pipeline.py           # Complete RAG pipeline
│   ├── graph.py              # LangGraph orchestration
│   └── evaluator.py          # Ragas + custom metrics
│
├── scripts/
│   ├── setup_validation.py   # Environment validation
│   ├── build_index.py        # Build FAISS index
│   ├── run_rag.py            # Run RAG queries
│   ├── eval_rag.py           # Evaluate system
│   └── compare_pipelines.py  # Benchmark different configs
│
├── tests/
│   ├── test_utils.py
│   ├── test_retriever.py
│   └── test_pipeline.py
│
├── docs/
│   ├── theory/
│   │   ├── 00_project_overview.md
│   │   ├── 01_embeddings.md
│   │   ├── 02_retrieval.md
│   │   ├── 03_reranking.md
│   │   └── 04_metrics.md
│   └── experiments/
│       └── ablation_study.md
│
├── data/                     # Raw documents
├── index/                    # FAISS indices
├── outputs/                  # Evaluation results
├── logs/                     # Application logs
│
├── requirements.txt
├── .gitignore
├── README.md
└── LICENSE
```

## Installation

### Prerequisites
- Python 3.10+
- CUDA 12.1+ (for GPU acceleration)
- 16GB+ GPU memory (recommended for Mistral-7B)

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/advanced-rag.git
cd advanced-rag
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Validate environment**
```bash
python scripts/setup_validation.py
```

This will check:
- Python version
- CUDA availability
- Required packages
- GPU memory
- FAISS GPU support
- Directory structure

## Quick Start

### 1. Download Dataset
```python
from datasets import load_dataset

# Example: SQuAD 2.0
dataset = load_dataset("squad_v2")
```

### 2. Build Index
```bash
python scripts/build_index.py \
    --data_path data/squad_v2 \
    --output_path index/squad_faiss.index \
    --embedding_model BAAI/bge-large-en-v1.5
```

### 3. Run Query
```bash
python scripts/run_rag.py \
    --query "What is the capital of France?" \
    --index_path index/squad_faiss.index \
    --top_k 5
```

### 4. Evaluate
```bash
python scripts/eval_rag.py \
    --test_set data/squad_v2_test.json \
    --output_dir outputs/evaluation
```

## Usage Examples

### Basic RAG Query
```python
from src.pipeline import RAGPipeline

pipeline = RAGPipeline(
    index_path="index/squad_faiss.index",
    embedding_model="BAAI/bge-large-en-v1.5",
    llm_model="mistralai/Mistral-7B-Instruct-v0.2",
    use_reranker=True
)

result = pipeline.query("What is quantum entanglement?")
print(result["answer"])
print(f"Sources: {result['sources']}")
```

### Custom Pipeline Configuration
```python
config = {
    "retrieval": {
        "top_k": 20,
        "use_hybrid": True,
        "bm25_weight": 0.3
    },
    "reranking": {
        "model": "BAAI/bge-reranker-base",
        "top_n": 5
    },
    "generation": {
        "max_tokens": 512,
        "temperature": 0.7
    }
}

pipeline = RAGPipeline.from_config(config)
```

## Evaluation Results

| Pipeline          | Recall@5 | nDCG  | Faithfulness | Latency (s) |
|-------------------|----------|-------|--------------|-------------|
| Baseline (LLM)    | 0.00     | -     | 0.45         | 0.8         |
| Dense Only        | 0.42     | 0.36  | 0.67         | 1.2         |
| Hybrid (BM25+Dense)| 0.49     | 0.44  | 0.72         | 1.5         |
| With Re-ranker    | 0.61     | 0.57  | 0.81         | 2.1         |

*Results on SQuAD 2.0 validation set*

## Development

### Running Tests
```bash
pytest tests/ -v --cov=src
```

### Code Formatting
```bash
black src/ tests/ scripts/
flake8 src/ tests/ scripts/
```

### Type Checking
```bash
mypy src/
```

## Theoretical Background

Comprehensive documentation on the theory behind each component:

- [Embeddings & Semantic Search](docs/theory/01_embeddings.md)
- [Retrieval Strategies](docs/theory/02_retrieval.md)
- [Re-ranking Techniques](docs/theory/03_reranking.md)
- [Evaluation Metrics](docs/theory/04_metrics.md)

## Performance Optimization

### GPU Optimization
- Use `faiss-gpu` for vector search
- Enable mixed precision (fp16) for LLM inference
- Batch processing for embeddings

### Memory Management
```python
from src.utils import GPUManager

# Clear cache between runs
GPUManager.clear_cache()

# Monitor memory
print(GPUManager.get_gpu_memory_info())
```

## Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

MIT License - see [LICENSE](LICENSE) for details

## Citation

If you use this code in your research, please cite:

```bibtex
@software{advanced_rag_2024,
  author = {Your Name},
  title = {Advanced RAG System},
  year = {2024},
  url = {https://github.com/ClementMathe/advanced-rag}
}
```

## Acknowledgments

- BGE embeddings by BAAI
- LangChain & LangGraph by LangChain AI
- Ragas evaluation framework
- Mistral AI for Mistral-7B

## Contact

- GitHub: [@ClementMathe](https://github.com/ClementMathe)
- Email: clementmathe1@gmail.com.com

---

**Status**: Active Development

Last Updated: December 2025
